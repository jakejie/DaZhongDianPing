# 大众点评商户数据爬虫

## 环境部署/配置
    1.配置setting文件中相关参数
    2.执行pipelines文件 生成所需表格
    3.其他


## 第零部分 获取所有城市
## 第一部分 获取所有分类下的商户列表
## 第二部分 根据获取到的商户列表获取商户详情页数据
## 第三部分 再次获取在第二步中没有获取到详情页的商户

### 0.获取所有城市
可配置使用代理执行 返回所有城市数据
      ```cd DaZhongDianPing/spiders```
      执行```python3 cityInfo.py```
    
### 1.获取所有分类下的商户列表
   在外面文件夹执行程序 ```python3 crawl_dianping.py```
#### 代码逻辑
    1.遍历所有城市
    2.获取城市下的所有分类
    3.获取当前分类下的下一级地区
    4.处理当前分类下当前地区的商户列表 若页码大于50 则继续获取下一级分类
    5.若无法获取页码 则默认页码为49（实际上可能是只有一页）
    6.处理商户列表页 并获取下一页地址 进行翻页
    7.可配置是否直接获取商户详情页数据 参数：CRAWLDETAIL
    8.CRAWLDETAIL设置为Ture时 直接获取详情页 否则 没有详情页数据
    9.注意：
        1.设置代理中间件 middlewares.py中设置ProxyMiddleware
        2.用过阿布云代理和讯代理 用的次数多了 质量整体下降
        3.对返回response进行检查
        4.经常会返回空字符串或者空类型 做好异常处理
    10.pipeline的处理
        1.使用了sqlalchemy的ORM进行数据存储
        2.定义了BasicInfo模型（实际上你看到两个 是因为我在处理第二步的时候不是使用的更新操作 而是重新插入了一张表）
        3.你下载使用的时候 可能需要更改pipeline文件
        
    
    
    
    
    

### 2.读取商户列表 获取商户详情页数据
   在外面文件夹执行程序 ```python3 crawl_business.py```

#### 代码逻辑
    1.从第一步中写入的商户列表的表中读出一定数量的商户列表数据
    2.遍历
    3.获取详情页数据
    4.注意：
        1.start_requests函数中 设置了偏移量以及每次提取数量
        2.因为数据量庞大（千万级） 不用一次性把所有数据读出来 会卡死的
        3.使用代理IP 这一步使用的代理IP 但是质量吧 比较低 100W数据花了4天
    

### 3.获取漏网之鱼
   在外面执行文件 ```python3 business_update.py```